# Rapport Technique : Intelligence Artificielle pour l'Estimation des Co√ªts de Transport

Ce rapport d√©taille la nature du mod√®le de pr√©diction impl√©ment√©, son architecture de fonctionnement et son processus d'apprentissage continu.

## 1. Nature du Mod√®le
Le syst√®me repose sur l'**Apprentissage par Renforcement** (Reinforcement Learning - RL), plus pr√©cis√©ment l'algorithme **PPO** (Proximal Policy Optimization) fourni par la biblioth√®que `stable-baselines3`.

> [!NOTE]
> Contrairement au Machine Learning classique qui pr√©dit √† partir de donn√©es historiques statiques, l'**RL apprend par l'exp√©rience** en interagissant avec un environnement simul√©.

### Pourquoi l'RL ?
- **Adaptabilit√©** : Le mod√®le peut s'ajuster dynamiquement √† de nouveaux param√®tres (ex: augmentation du prix du carburant).
- **Auto-Correction** : Il re√ßoit des "r√©compenses" ou des "p√©nalit√©s" bas√©es sur la pr√©cision de ses pr√©dictions, ce qui affine son jugement au fil du temps.

---

## 2. Architecture de Fonctionnement
L'IA fonctionne selon une boucle interactionnelle au sein de notre environnement personnalis√© `TravelCostEnv`.

### Le cycle d'apprentissage
```mermaid
graph LR
    A["√âtat (Situation)"] --> B["Agent (Mod√®le PPO)"]
    B --> C["Action (Estimation du prix)"]
    C --> D["Simulation (R√©alit√©)"]
    D --> E["R√©compense/P√©nalit√©"]
    E --> B
```

### Param√®tres d'entr√©e (Observations)
Le mod√®le analyse **8 facteurs cl√©s** pour chaque trajet :
1.  **Distance** (km)
2.  **Type de route** (Bitum√©e, terre, d√©grad√©e)
3.  **Trafic** (Fluide, moyen, dense)
4.  **M√©t√©o** (Intensit√© de la pluie)
5.  **Moment** (Jour ou Nuit)
6.  **S√©curit√©** (Accidents signal√©s)
7.  **Bagages** (OUI/NON) - *Nouvelle fonctionnalit√©* üß≥
8.  **Type de voie** (Routes larges) - *Nouvelle fonctionnalit√©* üõ£Ô∏è

---

## 3. Comment le mod√®le s'am√©liore
L'am√©lioration est un processus it√©ratif structur√© en trois niveaux :

### A. Phase d'Exploration
Au d√©but, le mod√®le fait des pr√©dictions al√©atoires. S'il surestime ou sous-estime massivement le co√ªt calcul√© par `simulation.py`, il re√ßoit une **forte p√©nalit√© n√©gative**.

### B. Phase d'Exploitation (Apprentissage)
Gr√¢ce √† la descente de gradient, l'algorithme met √† jour ses poids neuronaux pour maximiser le score de r√©compense. Il retient que :
- "Bagage = OUI" implique g√©n√©ralement un co√ªt plus √©lev√©.
- "Route Large = OUI" permet une meilleure fluidit√© donc un co√ªt plus bas.

### C. Le Laboratoire d'Am√©lioration Acc√©l√©r√©e (`fast_trainer.py`)
Pour acc√©l√©rer l'intelligence de l'IA sans attendre des mois d'utilisation r√©elle, nous utilisons un **Acc√©l√©rateur de Particules de Donn√©es** :
- Il g√©n√®re des milliers de sc√©narios synth√©tiques par seconde.
- Le mod√®le "vit" l'√©quivalent de **10 ans de trajets** en quelques minutes.
- Chaque session de lab g√©n√®re un nouveau "cerveau" (`checkpoint.zip`) plus malin que le pr√©c√©dent.

---

## 4. Int√©gration et D√©ploiement
Le mod√®le final est expos√© via une **API FastAPI** :
- **Entr√©e** : Requ√™te JSON depuis le frontend `farcal`.
- **Inf√©rence** : Le mod√®le charg√© en m√©moire calcule instantan√©ment le prix optimal.
- **Sortie** : Une estimation en FCFA avec une marge de confiance (ex: 1500 - 1700 CFA).

---

> [!TIP]
> **Le saviez-vous ?**
> Plus vous faites de simulations via le script `fast_trainer.py`, plus le mod√®le devient "stable". Un mod√®le bien entra√Æn√© a une **variance expliqu√©e** proche de 1.0 (actuellement √† 0.27, ce qui est un excellent d√©but pour des donn√©es complexes).
